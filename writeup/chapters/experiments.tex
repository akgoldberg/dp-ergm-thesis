 Because the convergence properties and accuracy of non-private inference methods for ERGMs are primarily studied via empirical testing, we evaluate our proposed methods for differentially private inference experimentally. We conduct a battery of tests, both on synthetic graphs drawn from a variety of ERGM models and on a high school friend network with nodes labeled by the sex and race of students. Comparing the noise-addition of our method against private bounding of local sensitivity and the inference quality against both private bounding of local sensitivity and randomized response, we demonstrate that our method gives better utility at low privacy budgets of around $\eps =1$ compared to competing methods. Then, we compare the inference quality of our proposed methods against non-private inference for the cases of private labels with edge-level privacy and the case of node-level privacy to demonstrate the viability of inference under these stronger privacy constraints.
 
 \section{Experimental Setup}
 
 \subsection*{Data}
 	We test our proposed methods on networks drawn from $3$ different ERGM models using the alternating structural sufficient statistics introduced in $\Cref{sec:alt_stats}$ so the probability distribution has the form:
 	$$\Pr(x | \theta) = \exp\left\{\theta_1 E(x) + \theta_2 u_\lambda^{(s)}(x) + \theta_3 u_\gamma^{(t)}(x)  + \theta_4 u_\gamma^{(p)}(x) - \psi(\theta)  \right\} $$
 	 with the following parameters for each model:
 	\begin{table}[!ht]
 		\label{table:ergm_params}
 		\centering
 		\begin{tabular}{|c|c|c|c|c|}
 			\hhline{|=====|}
 			Model & $\theta_1$ (Edges) & $\theta_2$ (Stars) & $\theta_3$ (Triangles) & $\theta_4$ (Two-Paths) \\ \hline
			1 & -4.6 & 0.0 & 1.0 & 0.0  \\
			2 & -4.6 & 0.0 & 2.0 & -0.1 \\
			3 & -4.6 & 2.0 & 2.0 & -0.5 \\
			\hhline{|=====|}
 		\end{tabular}
 	\caption{Parameters of Synthetic Networks}
 	\end{table}
 
 	 We fix the edge parameter at $-4.6$, because in the absence of any other sufficient statistics this corresponds to a $G(n,p)$ model with $p = 1\%$, leading to sparse networks. Then, the other parameters are chosen based on reasonable choices of these parameters from analyses of real network data, with Model $1$ being a simple model adding only the alt $k$-triangle parameter and Model $3$ being the most complex including all structural parameters. 
 	 
 	 We simulate networks using the sampling method detailed in \Cref{sec:sim_ergm} for each of the these models for graphs on $n$ nodes with $n$ ranging from $100$ to $1000$ we sample. For each $n$ we draw $50$ networks each. Then, on average, networks drawn from Model $1$  have edge density (proportion of possible edges present) converging to roughly $6\%$ on average, Model $2$ has edge density converging to roughly $1\%$ and
 	 Model $3$ has edge density converging to roughly $0.5\%$ on average. Thus, we study networks with a range of densities, while focusing on sparse networks, since our proposed algorithms are expected to work well for sparse networks and many real-world networks are sparse. Looking at the max degree of networks as well as the triangle count demonstrates that these three models lead to distinctive structure in the networks:
 	
 	\begin{figure}[!h]
 		\centering
 		\includegraphics[width=0.95\textwidth]{samples_summary.png}
 		\caption{Degree of simulated networks (with shading indicating max and min over the $50$ samples and dashed line showing median degree) along with the mean triangle count for simulated networks.}
 	\end{figure}
 
 	 Model $1$ has higher degree and much higher triangle count than Model $2$ or $3$, while Model $2$ has about three times as many triangles as Model $3$. Additionally, all the models put high probability on networks with much more complex structure than the basic $G(n,p)$ model with $p = 1\%$, for which  drawn networks tend to have only one tenth the number of triangles as Model $3$ despite having roughly the same edge density.  
 	
 	 In addition to the simulated networks, we test our methods on a network known as the Faux Mesa Network, which is a publicly available network released by sociologists who studied high schools in the Southwestern United States by surveying the entire student body and formulated a social network based on this research with $205$ nodes corresponding to students and $203$ undirected edges representing friendships between students. \cite{ergm} The released network that we use was generated by the sociologists who collected the data by fitting an ERGM (using standard non-private inference) to the  dataset and then releasing a network sampled from the fitted distribution. This indicates that privacy was a concern for the underlying network data. However, since the synthetic network maintains the interesting structural properties of the underlying dataset, performing inference over this dataset still permits us to ask whether researchers would reach the same conclusions under our proposed differentially private inference methods as they would performing non-private inference. The network is quite sparse with an edge density of $\approx1\%$, suggesting that it is well-suited to applying restricted sensitivity. Further, the nodes in the network representing students are labeled with `Race' and 'Sex' enabling us to model the effect nodal attributes of the network on structure.
 	 
 	 \begin{figure}[!h]
 	 	\centering
 	 	\includegraphics[width=0.5\textwidth]{faux_mesa_viz.png}
 	 	\caption{The Mesa High School Friendship Network with node colors representing Race and node border colors representing Sex.}
 	 \end{figure}


\subsection*{Implementation}
\begin{itemize}
	\item All of our code is written in the \texttt{R} programming language, with the procedure for sampling networks used for inference built on top of the packages \texttt{ergm} \cite{ergm} and \texttt{Bergm} \cite{Bergm}, which support non-private inference over exponential random graph models.
	\item In order to explore a broad parameter space and compare a variety of approaches, the experiments ran in parallel on Harvard's Odyssey computing cluster. To speed up individual experiments (which was particularly important during initial debugging), the  inference step exploited thread level parallelism  on the 32-core machines.
	\item Unless otherwise specified, we give overall privacy budget (in terms of $\eps$ and $\delta$), which is evenly split between sufficient statistics needed for inference, employing the composition property of differential privacy.
\end{itemize}
 
 \section{Edge-Adjacency Model}
 
For edge-level privacy, we compare our proposed method against two alternative methods, the private bound on local sensitivity approach suggested by Lu and Miklau \cite{LM14} and the randomized response method of Karwa et al. \cite{KKS17} Since the private local sensitivity bound also adds noise to the sufficient statistics and then performs inference like our restricted sensitivity-based method, we first compare the added noise between these methods. We find that restricted sensitivity allows for much lower noise addition, especially at small values of $\eps$. Then, we evaluate how this difference in noise addition impacts the quality of inference. 

We compare differentially private inference between all three methods by looking at KL-divergence between the privately learned ERGM distribution implied by taking posterior means as parameter estimates and the ground truth ERGM distribution. For the three synthetic networks, we take ground-truth to be the true parameters of the ERGMs from which we drew synthetic data (specified in Table \ref{table:ergm_params}) while for the Mesa high school data we learn parameters non-privately and take these to be our best guess of true parameters.  KL-divergence measures how different two probability distributions are, so low KL-divergence suggests that we have performed useful inference. For two ERGM distributions defined by their respective parameter vectors $\theta_a$ and $\theta_b$, KL-divergence from $\theta_b$ to $\theta_a$ is given by:
$$KL\left({\theta}_a || {\theta}_b\right) = \E_{{\theta}_a} \left[\log \frac{p(X | \theta_a)} {p(X | \theta_b) } \right] = \sum_{x \in \mathcal{G}_n} p(x | \theta_a) \log \frac{p(x | \theta_a)} {p(x | \theta_b)}$$
which is straightforward to estimate using MCMC methods detailed in \cite{HG10}. Additionally, we look at the root-mean-square error, or RMSE, between privately estimated parameters and ground-truth parameters to assess how different the learned parameters are from ground-truth. By looking at both RMSE and KL-divergence we can characterize how well we accomplish both the goal of enabling researchers to reach valid conclusions about the dataset (by looking at parameter values) and at drawing synthetic networks from the fitted ERGM (which requires that the learned distribution be similar to the true distribution.)
 
 \subsection{Noise Addition Comparisons}
 
 For noise addition comparison, we test the difference in noise added to sufficient statistics for the three synthetic networks and all values of $n$. For each of the $50$ networks of size $n$, we draw noise $50$ times based on each method resulting in $2500$ simulated noise draws for each network. We compute the four sufficient statistics (edges, alt-$k$-star, alt-$k$-triangle, and alt-$k$-two-path) with privacy levels per-statistic of $\eps = 0.1, 0.5, 1.$ and display results below for $\eps = 0.5$. For the private bounding of local sensitivity, which can only guarantee $(\eps, \delta)$-DP, we take $\delta = 10^{-6}$ per statistic, in order for the privacy guarantee to be comparable to that of pure $\eps$-differential privacy. 
 
 Further, we test varying degree cutoffs $k$ of the restricted degree hypothesis. Lower (more aggressive) setting of $k$ allows for less Laplace noise to be added since the restricted sensitivity is smaller. However, if $k$ is lower than the true degree of the network, then the projection to the space of networks of degree $k$, requires removing edges from the network, introducing bias into the released sufficient statistics.  Since mean square error is variance plus squared bias, using RMSE captures both the error from projection bias and from Laplace noise addition. Specifically, we test three choices of $k$ representing an aggressive setting, a somewhat aggressive but more reasonable setting and a fairly conservative setting respectively:
\begin{enumerate}
	\item Take $k$ equal to the minimum degree over the $50$ networks drawn from a given model, which may allow for low Laplace noise, but at the expense of potentially high bias induced by truncating many nodes.
	\item Take $k$ to be the median degree over the $50$ networks, allowing relatively low scale of Laplace noise, while also introducing limited bias since the edge-level projection  requires removing edges from nodes that have degree higher than $k$ of which we expect there to be relatively few.
	\item Take $k$ to be a conservative estimate of $1.5$ times the maximum degree of the $50$ networks drawn from a given model.
\end{enumerate}

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.49\textwidth]{noise/model1tribrief.png}
	\includegraphics[width=0.49\textwidth]{noise/model1twopbrief.png}\\
		\includegraphics[width=0.49\textwidth]{noise/model2tribrief.png}
	\includegraphics[width=0.49\textwidth]{noise/model2twopbrief.png}\\
		\includegraphics[width=0.49\textwidth]{noise/model3tribrief.png}
	\includegraphics[width=0.49 \textwidth]{noise/model3twopbrief.png}\\
	\includegraphics[width=0.35\textwidth]{noise/legend.png}
	
	\caption{Comparison of added noise with Laplace mechanism and $\eps = 0.5$ for private bounding of local sensitivity vs.restricted sensitivity with different values of $k$ for the restricted degree hypothesis.}
\end{figure}

As the 

 \subsection{Inference Comparisons}
 
 \subsection{Mesa High School Friend Network Data}
 
 \section{Edge-Adjacency Model with Private Labels}
 
 \section{Node-Adjacency Model}
 