\section{Motivation}

Networks are ubiquitous both as subjects of scientific study and as fixtures of everyday life. From social networks like Facebook to webs of financial transactions, mobile phone call records, and communications via email, rich network data constitutes a popular subject for statistical inquiry in a broad range of disciplines. However, due to the interconnected nature of the data, protecting the privacy of participants in a network while conducting statistical analysis can be difficult. A number of recent examples highlight the challenges of keeping network data private:

\textbf{The Cambridge Analytica Case} \cite{nytimes}:
In late March 2018, revelations emerged that a political consulting firm, Cambridge Analytica, had harvested over $50$ million user profiles off Facebook allowing them to build psychological profiles of a vast portion of the American electorate. Only $270,000$ users actually consented to give Cambridge Analytica access to their profile information via an online survey. However, by leveraging users' friend networks, it was possible for Cambridge Analytica to violate the privacy of a much larger number of people. 


\textbf{``Gaydar''} \cite{JM09}:
Consider an individual on Facebook who does not publicly disclose their sexual orientation, presumably because they wish this data to be kept private. By analyzing the proportion of this user's friends who publicly reveal being gay, it is possible to learn with high accuracy whether this user is gay or straight. In effect, then, a person's relationships along with publicly available data about their acquaintances, friends, and coworkers implicitly disclose private information about the person. 

\textbf{The Structure of Intimate Relationships} \cite{BK14}:
Even with no profile information made public, it is possible to identify the romantic partner of a Facebook user with high accuracy using only the structure of their friend network -- an intimate relationship is highly likely between individuals in the network who have many mutual friends, but whose friends have few mutual friends, a phenomenon known as ``dispersion.'' Therefore, suppressing identities of people in the network does not suffice to protect privacy, as the links within the network alone may reveal potentially sensitive, private information like a person's romantic partner.  

These examples speak to the fundamental difficulty of analyzing relational data while respecting the privacy of data holders -- network structure discloses an extensive amount of ostensibly private information about both the identity of participants and the nature of their relationships. Further, distinctive substructures of a network can make it relatively easy to reconstruct a naively anonymized network given auxiliary information: a number of proposed attacks on networks have demonstrated that an attacker with relatively little additional information may (with high probability) identify participants in any unlabeled network (\cite{BDK11},\cite{NS09}). Thus, statistical analysis of network data, while popular, is also problematic from a privacy standpoint. In response to this issue, a growing body of work seeks to answer the question: is it possible to protect the privacy of individuals included in a network dataset while enabling researchers to conduct useful analysis of network structure?

\subsection{Differential Privacy}

A promising direction for answering this question involves employing a rigorous and meaningful concept of privacy first proposed for analysis of tabular data: differential privacy \cite{DMNS06}. At a high level, differential privacy promises that the participation of any single individual in a dataset will not noticeably alter the results of an analysis of the dataset. The differential privacy guarantee  holds even if an adversary is equipped with arbitrary auxiliary information about the participants in a dataset. Indeed, differential privacy promises that if an adversary were to know the data of every other individual in a dataset, she still could not discover the private information of the final unknown participant in the dataset. 

Differential privacy provides a quantifiable notion of privacy, as it is operationalized by two small, non-negative parameters $\epsilon$ and $\delta$. The parameter $\eps$ captures the amount of privacy leaked by the analysis. As $\epsilon$ decreases, it becomes more difficult for an adversary to discern the private information of an individual in the dataset. The parameter $\delta$ specifies the probability of a potentially catastrophic privacy leakage. When $\delta = 0$, we speak of $\eps$-differential privacy. For $\delta > 0$, we provide $(\eps, \delta)$-differential privacy, which promises that an algorithm is $\eps$-differentially private with probability $1-\delta$, but permits an arbitrarily bad privacy leak with probability $\delta$. We may be comfortable with this relaxed concept of privacy if $\delta$ is vanishingly small (one in a million, for instance), so that the chance of a privacy leak is low. The quantifiability of differential privacy allows for rigorous study of the trade-off between privacy and utility in data analysis.

Differential privacy is a property of an algorithm -- it promises privacy by process. Only randomized algorithms meet the definition of differential privacy, suggesting that the algorithm must introduce noise in some way, either by perturbing the inputted data, the steps taken by the algorithm, or the output. One simple way to answer queries on a dataset under differential privacy constraints is by adding statistical noise drawn from an appropriately scaled Laplace distribution to the output of the query. This mechanism, called the Laplace mechanism, gives a general way of providing differential privacy, but requires one to compute the appropriate scale of Laplace noise to protect privacy for a specific query. In this work, we will employ the Laplace mechanism as a building block.

Protecting the privacy of individuals included in a network dataset requires identifying what aspects of the network should be considered private. A network abstractly represents relationships between various entities -- the entities are referred to as \emph{nodes} in the network (potentially with \emph{labels} specifying nodal attributes) and the links between entities are referred to as \emph{edges}. As the examples of ``Gaydar'' and romantic relationships on Facebook suggest, even if only node labels are considered private data, treating labels alone as private may not in itself preserve privacy, as the edge structure can reveal sensitive information about the labels. Furthermore, the goal may be to explicitly protect the relationships in a network, not just identities of participants. For instance, in a network of romantic partnerships the identity of participants may be public information, while the relationships could be sensitive. Thus, meaningful notions of privacy should seek to also protect the privacy of edges in a network.

In protecting the privacy of edges in a network there is ambiguity as to what granularity of privacy to provide. Differential privacy specifies that the ``participation'' of any single individual in a dataset should not alter the result of an analysis significantly. We could take ``participation'' to mean the inclusion of a single edge in the network and guarantee \emph{edge-level privacy} by protecting the privacy of any sensitive relationship in the network. Alternatively, we could protect the inclusion of a node and all edges incident to that node in the network, providing a much stronger privacy guarantee known as \emph{node-level privacy}. While node-level privacy offers a strictly stronger privacy guarantee than edge-level privacy, there may be cases where we are only concerned with protecting any single relationship in a network, not all of an individual's relationships. Further, node-level privacy may have an extensive cost in utility. Therefore, we consider both the notions of edge-level privacy and node-level privacy in our analysis.

\subsection{Inference on Network Data}

The goal of this work is to enable differentially private statistical inference for network data using a general class of models known as exponential random graph models or ERGMs. In contrast to simply computing statistics of a network -- like degree distributions or clustering coefficients -- a statistical model posits an explicit probability distribution over the space of possible networks, allowing researchers to study the distinctive structural properties of an instantiated network and the processes that gave rise to such structure. ERGMs are among the most commonly employed statistical models of network data, having been applied to a broad range of problems, including analysis of corporate management structures at Enron \cite{UHH13}, the demographics of high school friendships \cite{GKM09}, interactions between proteins in the human body \cite{EBB10}, and networks of neurons in the brain as people age  \cite{Sin+16}.

ERGMs are described fully by a vector of ``sufficient statistics'' computed on the network, which are generally aggregations of small substructures of the network like the number of edges or the number of triangles (groups of three connected nodes.) The ERGM associates a different parameter with each sufficient statistic, allowing researchers to understand the relative importance of different substructures in a network. Then, the goal of inference over an ERGM is to estimate parameters of the model that make the real-world network likely in the modeled probability distribution. 

%Then, there are two natural goals for differentially private inference over ERGMs:
%\vspace{-1.5em}
%\begin{enumerate}
%	\item Permit researchers to reach valid conclusions about network structure under differential privacy constraints.
%	\item Learn a model in a differentially private manner that puts high probability on networks with similar structure to the non-private network.
%\end{enumerate}
%\vspace{-1.5em}
%Fulfilling the first goal permits researchers to employ differentially private inference in rigorous study of network data. Meanwhile, the second (related) goal enables the private release of synthetic network data by sampling networks from an ERGM estimated in a differentially private manner. Currently, there exist ad-hoc methods for releasing anonymized data to researchers for statistical analysis. A common approach taken by statistical agencies is to fit an ERGM to a sensitive network dataset and then release a network drawn from the fitted model for use by researchers. \cite{ergm}. Our aim is to make rigorous the privacy guarantees of such an approach by guaranteeing differential privacy. 

There is a growing body of research on differentially private release of various statistics of networks, such as degree distributions (\cite{HLMJ09}, \cite{WNM16}), clustering coefficients \cite{WWZX12}, and counts of small subgraphs like triangles \cite{KRSY14} among many others. In comparison, there has been relatively little study of differentially private \emph{inference} over network data. Karwa and Slavkokvic propose a differentially private inference method for a specific class of ERGM known as the $\beta$-model \cite{KS16}. The $\beta$-model uses only the degree distribution as a sufficient statistic. While Karwa and Slavkokvic provide an elegant mathematical formulation of differentially private inference on this model, the $\beta$-model is used relatively infrequently in actual analyses, as it cannot capture many structures of interest in network data. For general ERGMs there exist two proposed methods. Lu and Miklau \cite{LM14} give an $(\eps, \delta)$-differentially private inference method based on adding Laplace noise to sufficient statistics of an ERGM, while Karwa et. al \cite{KKS17} propose an $\eps$-differentially private method based on randomly flipping edges of the underlying network inputted to inference. Both of these approaches work only for edge-level privacy with node labels taken to be public. Additionally, they permit accurate inference only for large privacy budgets (with $\eps$ taken to be greater than $3$ or $\delta$ taken to be $0.5$,) while in practice we want smaller privacy budgets with $\eps$ less than $1$ and $\delta$ taken to be very small (on the order of one in a million, for instance) \cite{N17}. In short, existing approaches to differentially private inference on network data only enable useful analysis for a weak privacy guarantee, namely, for settings where we use relatively large privacy budgets in the edge-privacy model with publicly known node labels. 

\section{Contributions}

Motivated by the goal of performing useful inference on network data while providing meaningful privacy guarantees, we propose a new framework for differentially private inference on ERGMs. We prove the privacy of our methods and then empirically evaluate their performance relative to alternative approaches and to non-private inference. There are three primary features of our proposed methods that move us towards the goal of practical differentially private inference for network data using ERGMs:
\begin{itemize}
	\item We enable accurate inference under edge-level privacy at \emph{smaller, more realistic, privacy budgets} than current methods.
	\item Unlike previously proposed methods, we permit differentially private inference under edge-level privacy with \emph{private node labels}, rather than treating labels as public.
	\item We suggest the first (to our knowledge) method for differentially private inference under the stronger notion of \emph{node-level privacy}.
\end{itemize}

Our approach takes advantage of the recently proposed machinery of ``restricted sensitivity'' (\cite{BBDS13}, \cite{KNRS13}) to perturb network data much less dramatically than existing methods for guaranteeing differential privacy. Restricted sensitivity exploits the observation that many real world networks are sparse: individuals in the network tend to have relatively few relationships compared to the size of the network. For instance, Facebook has over 2 billion users, but users have no more than a few thousand friends. While restricted sensitivity was initially proposed for edge-level and node-level differentially private release of statistics of networks, it has not been utilized for statistical inference over networks. We propose employing restricted sensitivity to elegantly leverage the sparsity common in real-world network data to perform useful private inference. In particular, we use the framework of restricted sensitivity to add statistical noise to computed sufficient statistics and then use these statistics to perform inference. Our inference method takes into account the noise introduced to sufficient statistics by the privacy mechanism to infer valid parameter estimates. 

To evaluate the performance of our methods, we conduct extensive experimentation on both synthetic network data and a real high school friendship network. Compared to prior work in differentially private inference on network data, which tests on networks of under $150$ nodes, we run inference experiments on larger networks of $200-300$ nodes. This is useful as current analyses using ERGMs increasingly look at larger networks, both because larger network datasets are becoming available and because greater computational power now enables inference on large networks. Further, by virtue of their size, larger networks are easier to keep private than smaller networks: when there are few participants, any one participant has a strong impact on an analysis, compared to a network with many participants. Thus, our experiments use networks large enough to guarantee meaningful levels of differential privacy while permitting useful inference.

 Our experiments on synthetic networks offer evidence that for modest privacy budgets of $\eps = 1$ or $2$, our method estimates parameters more accurately than existing methods. Then, we show in a case study on high school friend network data that our method allows researchers to accurately estimate parameters and standard errors for a budget of $\epsilon=2$, while existing approaches fail even for a larger budget of $\epsilon = 3$. This suggests that our approach enables researchers to reach reliable conclusions about the structure of real-world network data under substantive privacy constraints. Finally, we demonstrate the viability of our proposed methods for inference in the node-level privacy model by evaluating the noise addition of our proposed methods under a variety of assumptions. In summary, our experimental results suggest that our proposed restricted sensitivity-based methods allow for accurate inference under strong privacy guarantees in many settings where current methods do not, moving us closer to the goal of useful differentially private statistical modeling of network data.
 
\section{A Road Map}

The remainder of this thesis is structured as follows:

In \textbf{Chapter 2} we introduce the mathematical formulation of ERGMs. We define sufficient statistics of these models commonly used to capture network structure and then describe a standard non-private Bayesian inference method known as the Exchange Algorithm on which our private inference method is based.

In \textbf{Chapter 3} we give detailed background on differential privacy. We specify general mechanisms that meet the definition of differential privacy and useful properties of differential privacy and then describe the machinery of restricted sensitivity.

In \textbf{Chapter 4} we propose new methods for differentially private inference on ERGMs. In particular, we prove bounds on the restricted sensitivity of the common sufficient statistics of ERGMs introduced in Chapter 2 under both edge-level and node-level privacy, so that using mechanisms introduced in Chapter 4, we can provably protect differential privacy by adding statistical noise to sufficient statistics. We introduce a modified version of the Exchange Algorithm for Bayesian inference on ERGMs, which takes into account the noise of the privacy mechanism, thereby performing valid inference over the private posterior. Finally, we compare our method to existing work, giving high-level motivation for why we expect restricted sensitivity to outperform current methods.

In \textbf{Chapter 5} we empirically evaluate the performance of our proposed methods against current work in both the edge-level and node-level privacy models. We look at both the level of noise addition to sufficient statistics and the accuracy of parameter estimation for $3$ synthetic network models. In addition, we test inference on a friend network of high school students.  

In \textbf{Chapter 6} we conclude with suggestions for future work.