\section{Motivation}

Networks are ubiquitous both as subjects of scientific study and as fixtures of everyday life. From popular social networks like Facebook to webs of financial transactions, neuron connections in the brain, and communications via email, rich relational data constitutes a popular subject for statistical inquiry in a broad range of disciplines. However, due to the interconnected nature of the data, protecting the privacy of participants in a network while conducting statistical analysis can be difficult. A number of recent examples highlight the challenges of keeping network data private:

\textbf{The Cambridge Analytica Case}
In late March 2018, revelations emerged that a political consulting firm, Cambridge Analytica, had harvested over $50$ million user profiles off of Facebook allowing them to build psychological profiles of a vast portion of the American electorate. Only $270,000$ users had actually consented to give Cambridge Analytica access to their profile information via an online survey. However, by leveraging user's friend networks, it was possible for Cambridge Analytica violate the privacy of a much larger number of people. 
\cite{nytimes}

\textbf{``Gaydar''}
Consider an individual on Facebook who does not publicly disclose their sexuality, presumably because they wish this data to be kept private. By analyzing the proportion of this user's friends who publicly reveal being gay, it is possible to learn with high accuracy whether this user is gay or straight. In effect, then, a person's relationships along with publicly available data about their acquaintances, friends, and coworkers implicitly disclose private information about the person. \cite{JM09}

\textbf{The Structure of Intimate Relationships}
Even with no profile information made public, it is possible to identify the romantic partner of a Facebook user with high accuracy using only the structure of their friend network -- an intimate relationship is highly likely between individuals in the network who have many mutual friends, but whose friends have few mutual friends, a phenomenon known as ``dispersion''. Therefore, suppressing identities of people in the network does not suffice to protect privacy, as the links within the network alone may reveal potentially sensitive, private, information like a person's romantic partner.  \cite{BK14}

%\begin{example}[\emph{Reconstructing an ``Anonymous'' Network}]
%\label{ex:flickr}
%In 2011, the data science competition website \emph{Kaggle} released a dataset consisting of friendships from the on-line photo-sharing website Flickr, with all information about users withheld so that the network was purportedly anonymized. Additionally, $6\%$ of links between users were withheld, with the aim of the competition being to correctly predict the presence of these links in the network. The winning team was able to dramatically beat out the competition by deanonymizing the dataset -- comparing local relationship structures of the supposedly anonymous network against a version of the real Flickr network obtained by crawling over publicly available profiles to reveal both the identities of the users and the withheld links in the network.\cite{NSR11} 
%\end{example}

These examples highlight the fundamental difficulty of analyzing relational data while respecting the privacy of data holders -- network structure discloses an extensive amount of ostensibly private information about both the identity of participants and the nature of their relationships. Further, distinctive substructures of a network can make it relatively easy to reconstruct a naively anonymized network given auxiliary information: a number of proposed attacks on networks have demonstrated that an attacker with relatively little additional information may (with high probability) identify participants in any unlabeled network. (\cite{BDK11},\cite{NS09}). Thus, statistical analysis of network data while popular, is also problematic from a privacy standpoint. In response to this issue, a growing body of work seeks to answer the question:

\textbf{Is it possible to protect the privacy of individuals included in a network dataset while enabling researchers to conduct useful analysis of network structure?}

A promising direction for answering this question involves in employing a rigorous and meaningful concept of privacy first proposed for analysis of tabular data -- differential privacy \cite{DMNS06}. At a high level, differential privacy promises that the participation of any single individual in a dataset will not significantly alter the results of an analysis of the dataset. The differential privacy guarantee  holds even if an adversary is equipped with arbitrary auxiliary information about the participants in a dataset. Indeed, differential privacy promises that were an adversary to know the data of every other individual in a dataset, she still could not discover the private information of the final unknown participant in the dataset. 

Further, differential privacy provides a quantifiable notion of privacy, as it is parameterized by two small, non-negative values $\epsilon$ and $\delta$. The parameter $\eps$ captures the degree of privacy provided -- as $\epsilon$ increases, the likelihood that an adversary can discern the private information of an individual in the dataset grows. The parameter $\delta$ specifies the probability of a potentially catastrophic privacy leakage. When $\delta = 0$ we speak of $\eps$-differential privacy. For $\delta > 0$, we provide $(\eps, \delta)$-differential privacy, which promises that an algorithm is $\eps$-differentially private with probability $1-\delta$ but permits an arbitrarily bad privacy leak with probability $\delta$. We may be comfortable with this relaxed concept of privacy, if $\delta$ is vanishingly small (one in a million, for instance) so that the chance of a privacy leak is low. The quantifiability of differential privacy allows for rigorous study of the trade-off between privacy and utility in data analysis.

Protecting the privacy of individuals included in a network dataset requires identifying what aspects of the network should be considered private. A network abstractly represents relationships between various entities -- the entities are referred to as \emph{nodes} in the network (potentially with \emph{labels} specifying nodal attributes) and the links between entities are referred to as \emph{edges}. As the examples of ``Gaydar'' and romantic relationships on Facebook suggest, even if only node labels are considered private data, treating labels alone as private may not in itself preserve privacy, as the edge structure can reveal sensitive information about the labels. Furthermore, the goal may be to explicitly protect the relationships in a network, not just identities of participants. For instance, in a network of romantic partnerships the identity of participants may be public information, while the relationships could be sensitive. Thus, meaningful notions of privacy should seek to protect the privacy of edges in a network.

In protecting the privacy of edges in a network there is ambiguity as to what granularity of privacy to provide. Differential privacy specifies that the ``participation'' of any single individual in a dataset should not alter the result of an analysis significantly. We could take ``participation'' to mean the inclusion of a single edge in the network and guarantee \emph{edge-level privacy} by protecting the privacy of any sensitive relationship in the network. Alternatively, we could protect the inclusion of a node and all edges incident to that node in the network, providing a much stronger privacy guarantee known as \emph{node-level privacy}. While node-level privacy offers a strictly stronger privacy guarantee than edge-level privacy, there may be cases where we are only concerned with protecting any single relationship in a network, not all of an individual's relationships. Therefore, we consider both the notions of edge-level privacy and node-level privacy in our analysis.

\section{Our Contributions}

The primary goal of this work is to enable differentially private statistical inference for network data using a general class of models known as exponential random graph models or ERGMs. In contrast to simply computing statistics on a network -- like degree distributions or clustering coefficients -- statistical modeling of a network posits an explicit probability distribution over the space of possible networks, allowing researchers to study the distinctive structural properties of an instantiated network and the processes that gave rise to such structure. ERGMs are one of the most commonly employed statistical models of network data, having been applied to a broad range of problems, including analysis of interactions between proteins in the human body \cite{EBB10}, networks of neurons in the brain as people age  \cite{Sin+16}, corporate management structures at Enron  \cite{UHH13}, and the demographics of high school friendships \cite{GKM09}. The goal of inference over an ERGM is to estimate parameters of the model that make a real-world network most likely over the modeled probability distribution. Then, there are two natural goals for differentially private inference over ERGMs:
\vspace{-1.5em}
\begin{enumerate}
	\item Permit researchers to reach valid conclusions about network structure under differential privacy constraints.
	\item Learn a model in a differentially private manner that puts high probability on networks with similar structure to the non-private network.
\end{enumerate}
\vspace{-1.5em}
Fulfilling the first goal permits researchers to employ differentially private inference in rigorous study of network data. Meanwhile, the second (related) goal enables the private release of synthetic network data by sampling networks from an ERGM estimated in a differentially private manner. Currently, there exist ad-hoc methods for releasing anonymized data to researchers for statistical analysis. A common approach taken by statistical agencies is to fit an ERGM to a sensitive network dataset and then release a network drawn from the fitted model for use by researchers. \cite{ergm}. Our aim is to make rigorous the privacy guarantees of such an approach, by guaranteeing differential privacy. 

There is a growing body of research on differentially private release of various statistics of networks, such as degree distributions (\cite{HLMJ09}, \cite{WNM16}), clustering coefficients \cite{WWZX12}, and counts of small subgraphs like triangles \cite{KRSY14} among many other work. In comparison, there has been relatively little study of differentially private inference over network data. Karwa and Slavkokvic propose a differentially private inference method for a specific class of ERGM known as the $\beta$-model. The $\beta$-model uses only the degree distribution as a sufficient statistic \cite{KS16}. While they provide an elegant mathematical formulation of differentially private inference on this model, the $\beta$-model is used relatively infrequently in actual analyses, as it cannot capture many structures of interest in network data. For general ERGMs there exist two proposed methods. Lu and Miklau \cite{LM14} give an $(\eps, \delta)$-differentially private inference method based on perturbing sufficient statistics of an ERGM while Karwa et. al \cite{KKS17} propose an $\eps$-differentially private method based on perturbing the underlying network. Both of these approaches work only for edge-level privacy with node labels taken to be public. Additionally, they only allow for accurate inference under large privacy budgets (with $\eps$ taken to be greater than $3$ or $\delta$ taken to be $0.5$,) while in practice we want much smaller privacy budgets with $\eps$ less than $1$ and $\delta$ taken to be very small (on the order of one in a million, for instance) \cite{N17}. As we verify through a battery of experiments, neither of these approaches allow for private inference in the edge-privacy model with desirable levels of privacy. In contrast, our proposed methods allow for accurate edge-level $\eps$-differentially private inference with low privacy budgets. Further, we are the first to suggest methods for inference under node-level privacy and for inference which treats labels as private. In short, we provide a comprehensive study of differentially private inference using ERGMs under a number of different granularities of privacy and at more realistic privacy budgets than current work. We make three main contributions towards the goal of practical differentially private inference for network data using ERGMS:
\begin{itemize}
	\item We enable accurate inference \emph{under edge-level privacy at lower, more realistic, privacy budgets} than current methods.
	\item Unlike, previous proposed methods, we permit differentially private inference under edge-level privacy with \emph{private node labels}, rather than treating labels as public.
	\item We suggest the first (to our knowledge) method for differentially private inference under the stronger notion of \emph{node-level privacy}.
\end{itemize}

Our methods take advantage of the recently proposed machinery of ``restricted sensitivity'' (\cite{BBDS13}, \cite{KNRS13}) to perturb network data much less dramatically than existing methods for guaranteeing differential privacy. Restricted sensitivity exploits the observation that many real world networks are sparse: individuals in the network tend to have relatively few relationships compared to the size of the network. For instance, Facebook has over 2 billion users, but users have no more than a few thousand friends. While restricted sensitivity was initially proposed for edge-level and node-level differentially private release of statistics of networks, it has not been utilized for statistical inference over networks. We propose employing restricted sensitivity to elegantly leverage the sparsity common in real-world network data to perform useful private inference. 

Through extensive experimentation on both synthetic networks and a high school friendship network, we demonstrate the improved utility of our approach over existing methods in the edge-level privacy model and the viability of our approach for inference with private labels and in the node-level privacy model.
 
\section{A Road Map}

In \textbf{Chapter 2} we...

In \textbf{Chapter 3} we...

In \textbf{Chapter 4} we...

In \textbf{Chapter 5} we...

In \textbf{Chapter 6} we...