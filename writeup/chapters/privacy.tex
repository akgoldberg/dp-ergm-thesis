We employ the framework of differential privacy to protect individuals' data while analyzing network data.  First, we provide the basic definition of differential privacy and mechanisms that meet this definition for general datasets, then explain specific problems that arise in applying these general definitions to network data, and finally detail the machinery of ``restricted sensitivity'' which we propose to use for differentially private inference over ERGMs.

\section{Basics of Differential Privacy}

\subsection*{Definitions}

Let $\mathcal{D}$ denote the space of all possible datasets. Then:

\begin{definition}
\label{def:neighbors}
Two datasets $x, x' \in \mathcal{D}$ are \emph{adjacent}, written as $x \sim x'$, if 
they differ in the record of one individual. For tabular data, this means that the datasets differ in a single row. 
\end{definition}

\begin{definition}
	\label{def:distance}
	The \emph{distance} between two datasets $x, x' \in \mathcal{D}$, denoted $d(x, x')$ is the minimum length of the sequence of datasets beginning with $x$ and ending with $x'$ such that every two consecutive datasets on the path are adjacent. So, two datasets are clearly adjacent, or neighboring, if $d(x, x') = 1$. 
\end{definition}

\begin{definition}[$\eps$-differential privacy \cite{DMNS06}] Let $\A$ be an algorithm over datasets in $\mathcal{D}$. Then $\A$ is \emph{$\eps$-differentially private} if for all $S \subseteq \text{Range}(\A)$ and for every pair of neighboring datasets $x, x' \in \mathcal{D}$,
	\begin{equation*}
	\Pr[\A(x) \in S] \leq e^{\eps} \Pr[\A(x') \in S]
	\end{equation*}
\end{definition}

Intuitively, differential privacy promises that the participation of any individual in a dataset does not significantly change the outcome of an analysis run on the dataset, limiting the potential harm (or benefit) to a data provider due to the inclusion of her data. Smaller values of $\epsilon$ correspond to stronger guarantees of privacy where $\epsilon = 0$ suggests that the algorithm does not learn anything from the data and therefore the algorithm is useless. Additionally, it is clear that no non-trivial deterministic algorithm satisfies $\eps$-differential privacy for any value of $\eps$. If $\A$ is deterministic and its output differs on at least two datasets, then there must be neighboring datasets such that the probability of a specific output is $0$ on one dataset and $1$ on the other, preventing the ratio between probabilities on this response from being bounded as required. Therefore, mechanisms that provide differential privacy will have to introduce some randomness, or noise, into their answers.

We can relax the definition of $\epsilon$-differential privacy to allow for a small probability of potentially catastrophic privacy leakage:

\begin{definition}[$(\eps, \delta)$-differential privacy  \cite{DMNS06}]
$\A$ is \emph{$(\eps, \delta)$-differentially private} if for all $S \subseteq \text{Range}(\A)$ and for every pair of neighboring datasets $x, x' \in \mathcal{D}$,
\begin{equation*}
\Pr[\A(x) \in S] \leq e^{\eps} \Pr[\A(x') \in S] + \delta
\end{equation*}
\end{definition}

It is immediate from the definition that $(\epsilon, \delta)$-differential privacy is equivalent to $\epsilon$-differential privacy  when $\delta = 0$. For $\delta > 0$,  however, $(\eps, \delta)$ guarantees that the mechanism is $\eps$-differentially private with probability $1-\delta$, but makes no promises about the privacy loss that occurs with probability $\delta$. Therefore, if $\delta$ is on the order of $\frac{1}{n}$ where $n$ is the size of the dataset, it is possible to satisfy $(\eps, \delta)$-DP by releasing a row of the data. Further, a mechanism that sometimes releases the entire dataset still satisfies $(\epsilon, \delta)$-DP.

\begin{example}
	An algorithm that selects at random one record in the dataset and exactly releases this record is $(\eps, \frac{1}{n})$-differentially private for any value of $\eps$.
\end{example}

\begin{example}
	An algorithm that releases the entire dataset with probability $\delta$ and a constant value with probability $1-\delta$ is $(\epsilon, \delta)$-differentially private for any value of $\eps$.
\end{example}
As these examples demonstrate, $(\epsilon, \delta)$-differential privacy only provides meaningful privacy for values of $\delta$ much smaller than $\frac{1}{n}$. In particular, $\epsilon$ should be taken to be ``cryptographically small'' (e.g.  take $\delta = \frac{1}{1,000,000}$ for networks over a few $100$ nodes.)

\subsection*{Properties}

One of the desirable properties of differential privacy is its immunity to \emph{post-processing} -- armed with the output of a differentially private mechanism, an analyst cannot degrade privacy any further without additional information about the private dataset. In the context of inference over ERGMs, this property suggests that after computing sufficient statistics of a model in a differentially private manner, inference using these sufficient statistics can be thought of as a post-processing step that does not further degrade privacy. Formally: 
	\begin{property}[Post-processing \cite{DMNS06}]
		If $\A$ is an $(\eps, \delta)$-differentially private algorithm, then for an arbitrary mapping $f$,  $f \circ \A$ is also $(\eps, \delta)$-differentially private. 	
	\end{property}

A second useful property of differential privacy is that multiple differentially private algorithms compose, so applying many differentially private algorithms to the same dataset still provides privacy, albeit with higher privacy loss. This allows for basic DP algorithms to be used as building blocks in more complicated algorithms and in particular to split a privacy budget across multiple private computations on the data. Specifically, basic composition states that the privacy loss incurred by running multiple DP algorithms on a dataset grows linearly:
	\begin{property}[Basic Composition \cite{DMNS06}]
		Let $\A_i$ be an  $(\eps_i, \delta_i)$-differentially private algorithm for $i \in [k]$. Then, the algorithm releasing the result of running all $k$ algorithms on the dataset $\A_{[k]}(x) = (\A_1(x),...,A_k(x))$ is $\left(\sum_{i=1}^k \eps_i, \sum_{i=1}^k\delta_i\right)$-DP.
	\end{property}



\subsection*{Mechanisms}

We now describe two simple mechanisms that satisfy differential privacy. First, we describe the Laplace Mechanism, which answers queries on a dataset in a differentially private manner by adding Laplace noise to queries. Then, we introduce Randomized Response, which  provides privacy by randomly perturbing the underlying dataset.

\subsubsection{Laplace Mechanism}

 We define a query to be a function mapping the dataset to a vector of real numbers, $f: \mathcal{D} \to \mathbb{R}^m$. Then the local sensitivity of a query on a dataset $x$ is the maximum $\ell_1$-norm of the difference in the query over neighbors of dataset $x$.

\begin{definition}[Local sensitivity]
The \emph{local sensitivity} of a query $f$ on a dataset $x$ is $$LS_f(x) = \max_{x' \sim x} ||f(x) - f(x')||_1$$
\end{definition}
The global sensitivity is the worst-case local sensitivity over all possible datasets:
\begin{definition}[Global sensitivity]
The \emph{global sensitivity} of a query $f$ is $$GS_f = \max_{x \in \mathcal{D}} LS_f(x)$$
\end{definition}

A basic result in differential privacy is that adding Laplace noise scaled to the global sensitivity provides differential privacy:
\begin{theorem}[Laplace mechanism \cite{DMNS06}]
	\label{thm:laplace}
	Let $f$ be a query on dataset $x$ with global sensitivity $GS_f$ and let \emph{$\Lap$} denote the zero-mean Laplace distribution\footnote{The Laplace distribution centered at $0$ with scale parameter $b$ has probability density function $p(x | b) = \frac{1}{2b} e^{-|x|/b}$ and the variance of the distribution is $\sigma^2 = 2b^2$.}. Then, the Laplace mechanism $\A_{L}$ that outputs 
	$$\A_L(x, f, \eps) = f(x) + (Y_1,...,Y_m)$$
	where $Y_i  \stackrel{i.i.d.}{\sim} \Lap\left(\frac{GS_f}{\eps}\right)$ is $\eps$-differentially private.
\end{theorem}
Note that the Laplace mechanism scales noise to the \emph{global sensitivity}. While it is tempting to calibrate noise to local sensitivity, this does not protect privacy, because the noise level may disclose information about the underlying dataset. However, we can add noise scaled to a smooth upper bound on the local sensitivity, namely a function $S$ that is larger than the local sensitivity for all datasets and for which $\ln(S(\cdot))$ is not too sensitive. The smoothness is parameterized by $\beta$, where $\beta$ depends on $\eps$ and $\delta$:
\begin{theorem}[Calibrating Noise to $\beta$-Smooth Upper Bound on Local Sensitivity \cite{NRS07}]
\label{thm:smooth}
A $\beta$-smooth upper bound on the local sensitivity of query $f$ is a function $S_{f, \beta}$ that satisfies:
\vspace{-0.2in}
\begin{enumerate}[(i)]
	\item $S_{f, \beta}(x) \geq LS_f(x) \quad \forall x \in \mathcal{D}$
	\item $S_{f, \beta} (x) \leq \exp\left\{- \beta d(x, x') \right\} S_{f, \beta}(x') \quad \forall x, x' \in \mathcal{D}$
\end{enumerate}
\vspace{-0.2in}
It is possible to satisfy $(\epsilon, \delta)$-differential privacy by adding Laplace noise scaled to $\frac{2S_{f,\beta}(x)}{\epsilon}$ where $\beta = -\frac{\eps}{2 \ln(\delta)}$ and $S_{f, \beta}$ is a $\beta$-smooth upper bound on $LS_f(x)$. It is possible to satisfy $\epsilon$-differential privacy by adding Cauchy noise\footnote{The Cauchy distribution with median $0$ and scale paramter $b$ has probability density function $p(x|b) = 1/(b \pi(1+(x/b)^2 ) )$. Roughly, the Cauchy distribution can provide $\eps$-DP because it has fatter tails than the Laplace distribution.} scaled to $\sqrt{2}S_{f, \beta}(x)$ where $\beta = \eps/\sqrt{2}$.
\end{theorem}

 Then, global sensitivity trivially satisfies this definition, but it is a very conservative bound on the local sensitivity. The smallest function $S$ to satisfy the definition of a $\beta$-smooth upper bound is known as the \emph{smooth sensitivity}:
\begin{definition}[Smooth Sensitivity \cite{NRS07}]
\label{def:smooth}
For query $f$ and dataset $x$, define the \emph{local sensitivity at distance of $t$} to be
$$LS_f^{(t)}(x) = \max_{\substack{x' \in \D:\\ d(x, x') \leq t|}} LS_f(x')$$
Then the \emph{smooth sensitivity} is
$$S^*_{f, \beta}(D) = \max_{t} e^{-t \beta}  LS^{(t)}(D)$$
The smooth sensitivity is the smallest $\beta$-smooth upper bound on the local sensitivity in the sense that for any other $\beta$-smooth upper bound $S$, $S^*_{f, \beta}(D) \leq S(D)$ for all datasets $D$. Thus, if we can compute the smooth sensitivity efficiently, then we can potentially add much less noise by calibrating to smooth rather than global sensitivity.
\end{definition}

%\begin{theorem}[Exponential mechanism \cite{MT07}]
	%Let $u(x,r)$ be a utility function that maps database-outcome pair $(x,r)$ to a real-valued score. Then, the Exponential mechanism $\A_{E}$ that produces a random outcome with probability:
	%$$\Pr(\A_E(x, u, \epsilon) = r) \propto \exp \left(\frac{\eps u(x,r)}{2 GS_u} \right)$$
	%is $\epsilon$-differentially private. The global sensitivity of the utility function is the maximum change in the utility function on any fixed outcome over a pair of neighboring datasets:
	%$$GS_u = \max_{r, x' \sim x} |u(x,r) - u(x', r)|$$
%\end{theorem}

\subsubsection{Randomized Response}

In contrast to the Laplace Mechanism, which perturbs the output of a query on a dataset, randomized response perturbs the underlying dataset by randomly introducing spurious data. A typical version of randomized response over binary data proceeds as follows:


For each bit in a dataset consisting of $\{ 0,1 \}$ values:
\vspace{-0.2in}
\begin{enumerate}
	\item Flip a biased coin with probability $p_1$ of heads.
	\item If tails, then record the bit truthfully.
	\item If heads, then flip a second biased coin with probability $p_2$ of heads and record $1$ if heads, $0$ if tails.
\end{enumerate}

A benefit of randomized response is that it can be employed while collecting data, by using the coin-flipping procedure to collect responses in a study. The method provides plausible deniability for respondents, so it may incentivize participation in surveys for sensitive information.  It is easy to verify that taking $p_1 = 2p$ and $p_2 = \frac{1}{2}$ yields the following simpler description:

\begin{theorem}[Binary Randomized Response \cite{War65},\cite{KKS17}]
	\label{thm:rr}
	Let $\mathcal{D} = \{0,1\}^n$ so $x \in \mathcal{D}$ consists of binary data. Then, randomized response flips each bit of $x$ with probability $p \in (0, \frac{1}{2})$ and releases the resulting noisy bits. This process provides $\eps$-differential privacy taking  $p \geq \frac{1}{e^\eps + 1}$.
\end{theorem}

 We may also define randomized response for a dataset where each row is drawn from a general data universe $\mathcal{U}$:

\begin{theorem}[General Randomized Response]
\label{thm:rr_gen} Consider dataset $x \in \mathcal{D} = \mathcal{U}^n$ and an algorithm which with probability $p$ for each row, replaces the row with an entry drawn uniformly at random from $\mathcal{U}$. Then, this algorithm is $\epsilon$-differentially private, taking $p \geq \frac{|\mathcal{U}| - 1}{e^{\eps} + |\mathcal{U}| - 1 }$.
\end{theorem}

\section{Edge-Level vs. Node-Level Adjacency}

We now turn to the question of how to define ``adjacency'' for graphs, as opposed to tabular data. We will define graphs abstractly in terms of vertex sets and edge sets, rather than as adjacency matrices in this section, as it makes the definitions easier to specify and more intuitive. There are two reasonable and widely used definitions of adjacency, which provide privacy at very different granularities and thus may be appropriate in different circumstances: 

\begin{definition}[Edge-level adjacency]
\label{def:edge_level}
We define two graphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$ to be \emph{edge-adjacent} if they have the same vertex set ($V_1 = V_2$) and they differ in only one edge ($\left| E_1 \triangle E_2   \right| = 1$). 
\end{definition}
Differential privacy with respect to edge-adjacency protects the privacy of individual relationships between nodes. Thus, edge-level privacy could protect a Facebook friendship with a controversial political leader. However, privacy at the edge-level could not promise to prevent an adversary from discerning whether an individual has mostly Republican or Democratic friends on Facebook. Such concerns motivate a stronger definition of neighboring graphs:

\begin{definition}[Node-level adjacency]
\label{def:node_level}
We define two graphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$ to be \emph{node-adjacent} if $G_1-v_i = G_2-v_i$ for some vertex $v_i$, where $G-v_i$ means deleting edges adjacent to node $v_i$.
\end{definition}

An additional consideration in defining adjacent graphs is how to account for labeled nodes. In the node-level case, labels are protected since removing a vertex and replacing it with a different vertex suggests changing the labeling on that vertex. For edge-level privacy, labels could be taken to be either public or private information.  There may be cases where the only sensitive information is the edges in the graph, not the identities of nodes (for instance, in a public social network, where people's identities may be readily searchable online, while their friendships are kept private.) However, in many settings, it seems preferable to protect the labels in addition to the relationships. Thus, letting there be some labeling function associated with a network that specifies a vector of nodal attributes for each node $\ell: V \to \mathbb{R}^m$, we define edge-level adjacency for labeled networks as follows:

\begin{definition}[Edge-level adjacency with private labels]
	\label{def:edge_level}
	We define two graphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$ with labeling functions $\ell_1$ and $\ell_2$ to be \emph{edge-adjacent with private labels} if they have the same vertex set ($V_1 = V_2$) and either they  differ in only one edge ($\left| E_1 \triangle E_2   \right| = 1$) or differ in one label ($\ell_1(v) \not = \ell_2(v)$ for exactly one vertex $v$.)
\end{definition}

%The \emph{distance} between two graphs $G_1$ and $G_2$, denoted $d(G_1, G_2)$ is the minimal length of the sequence of graphs beginning with $G_1$ and ending with $G_2$ such that every two consecutive graphs on the path are neighbors (with respect to either node-level  or edge-level adjacency). %To go from $G_1$ to $G_2$ we will need to either add or remove every edge in the symmetric difference between the edge sets of $G_1$ and $G_2$ (all edges that are not the same in the two graphs.)  Thus, under edge-level adjacency, we step from $G_1$ to $G_2$ by each edge, so $d(G_1, G_2) = |E_1 \triangle E_2|$. Under node-level adjacency, we can change all edges adjacent to a node at once, so in order to go from one graph to another, we can step through each node that touches edges that differ between the graphs. Therefore, the distance between two graphs is given by the size of the vertex cover of the symmetric difference graph (the graph including the edges in $E_1 \triangle E_2$): $d(G_1, G_2) = |VC(G_1 \triangle G_2)|$. Note that, finding the vertex cover of an arbitrary graph is NP hard, so computing the distance between two graphs under node-adjacency is an NP-hard problem\cite{BBDS13}.

\section{Restricted Sensitivity}
\label{sec:restricted_sensitivity}

 Node-level privacy constitutes a strictly stronger guarantee than edge-level privacy, but it is often much more difficult to perform accurate analysis under node-level privacy. For instance, consider computing the degree distribution on an $n$-node graph. The global sensitivity under edge-level adjacency is only $2$, since the degree of two nodes will change by $1$ due to the addition or removal of an edge. However, under node-level adjacency, removing or adding all edges to a node of degree $n-1$ would affect $n$ entries of the degree distribution, so the global sensitivity is $n$ and naive application of the Laplace mechanism would completely destroy the counts of the degree distribution. Furthermore, even under edge-level adjacency many statistics computed on networks have high global sensitivity. For instance, the count of triangles in a graph (which is used in the alternating $k$-triangle sufficient statistic in ERGMs) has global sensitivity $O(n)$ in the edge-level case, since a single edge could be the base of a triangle with each other node in the graph.
 
 The high global sensitivity of many graph statistics is particularly problematic for sparse graphs, where the noise completely overwhelms the true statistics. This is especially troubling, because sparsity is a characteristic of many real world networks. For instance, Facebook has billions of users, but  users tend to have on the order of 1000 friends or fewer. We can formalize the hypothesis that a graph is sparse by considering the \emph{degree} of the graph, the maximum degree of any of its nodes. If we hypothesize that all the graphs under consideration have limited degree, then the global sensitivity might be much lower over these limited-degree graphs than over all graphs on $n$ nodes. For example, considering the space of graphs with degree of at most $k << n$, the triangle count would have a much global sensitivity of $O(k)$ rather than $O(n)$ over the space of all graphs on $n$ nodes.
 
 If we were certain that the graphs under consideration always had limited degree, we could scale noise to the sensitivity over limited degree graphs. However, our hypothesis might be false, so adding noise assuming that the graph has limited degree would not protect privacy for an arbitrary graph.  Therefore, it is necessary to first project the graph into the space of limited degree graphs. If the limited degree hypothesis is true then the projection will not alter the graph at all, so the analysis is accurate up to the distortion of the noise-adding procedure. We formally define the \emph{limited degree hypothesis} as the space of graphs on $n$ nodes with degree $k$:
 
 \begin{definition}[Limited Degree Hypothesis]
 Let $\G_n$ be the space of graphs on $n$ nodes. Then, the limited degree hypothesis $\H_k$ is the set of graphs:
 $$\H_k = \{G = (V,E) \in \G_n : \text{deg}(v) \leq k, \forall v \in V \}$$
 \end{definition}
 
 Then, the restricted sensitivity is the global sensitivity of the query restricted to limited degree graphs:

\begin{definition}[Restricted sensitivity \cite{BBDS13}]
\label{def:restricted-sensitivity}
For a given notion of adjacency (either edge or node), we define the \emph{restricted sensitivity} of query $f$ over hypothesis $\H_k \in \G_n$ as
$$RS_f(\H) =\max_{\substack{G, G'\in \H_k:\\ G\sim G'}} ||f(G - f(G')||_1$$
\end{definition}

% % The restricted sensitivity is defined over all graphs in $\H$, not just two neighboring graphs. In general, the distance between two graphs defined by a sequence of graphs does not have to include only graphs belonging to $\H$.  However, if we use
%  For the limited degree hypothesis, the restricted sensitivity is bounded by the global sensitivity restricted to $\H_k$:

% \begin{lemma}[Restricted sensitivity for $\H_k$]
% 	\label{lemma:RS-Hk}
% 	For , the restricted sensitivity is bounded by 

% \end{lemma}
% \begin{proof}
% 	Note that if the distance between two $k$-degree graphs is $m$, then there must be a sequence of adjacent graphs (in either the node or edge adjacency formulation) all in $\H_k$ of length $m$ starting with the first graph and ending with the second graph. This holds by always removing edges in the sequence before adding edges.\footnote{Note that for an arbitrary hypothesis $\H$, however, the distance between two graphs in $\H$ may be realized only by a sequence that include graphs not in $\H$, which is why we give the general definition of restricted sensitivity as in \Cref{def:restricted-sensitivity}.} Now, consider two graphs $G_0, G_m \in \H_k$ with $d(G_0, G_m) = m$ and let $G_i \in \H_k, i \in [m]$ be a sequence of adjacent graphs ($G_i \sim G_{i+1}$) beginning with $G_0$ and ending with $G_m$. Then, applying the triangle inequality gives:
% 	\begin{align*}
% 	RS_f(\H_k) = \max_{G_0, G_m \in \H_k} \frac{|f(G_0) - f(G_m)|}{d(G_0, G_m)} & = \max_{G_0, G_m \in \H_k} \frac{| \sum_{i = 0}^{m-1}f(G_i) - f(G_{i+1})|}{m} \\
% 	& \leq \max_{G_0, G_m \in \H_k}  \frac{\sum_{i = 0}^{m-1}|f(G_i) - f(G_{i+1})|}{m} \\
% 	& \leq \max_{\substack{G, G'\in \H_k:\\ G\sim G'}} |f(G) - f(G')|
% 	\end{align*}
% \end{proof}

%In many cases, the difference between neighboring graphs in $\H_k$ given in \Cref{lemma:RS-Hk} will be much easier to bound than the general formulation of restricted sensitivity given in \cref{def:restricted-sensitivity} over $\H_k$. 
To protect privacy over arbitrary graphs, while calibrating noise to the restricted sensitivity rather than the global sensitivity, we require a projection $\mu: \G \to \H_k$. We can define the sensitivity of the projection in terms of how much it changes the distance by a multiplicative factor between any two adjacent graphs. In particular:

\begin{definition}[Local sensitivity of projection $\mu$ \cite{KNRS13}]
Define the local sensitivity of projection $\mu: \G_n\to \H_k$ on graph $G \in \G_n$ to be:
$$LS_\mu(G) = \max_{G' \sim G} d(\mu(G), \mu(G' )) $$ 
\end{definition} 

%A desirable property of this projection is ``smoothness'' which requires that two adjacent graphs are still close in distance after being projected to $\H$. We define a \emph{smooth projection} as:

%\begin{definition}[$c$-smooth projection \cite{BBDS13}]
%A projection $\mu: \G _n\to \H_k$ is \emph{$c$-smooth} if for any pair of adjacent graphs $G \sim G'$, $$d(\mu(G), \mu(G') )\leq c$$
%\end{definition}
%Since :

Then, the global sensitivity and smooth sensitivity can be defined as before. Now, if we can find a projection $\mu$, where it is possible to bound the global sensitivity by a small constant, so $\forall G \in \G_n: LS_{\mu} (G)\leq c$, then for any two neighboring graphs the effect of first projecting a graph to $\H_k$ before answering a query only increases global sensitivity by a multiplicative factor of $c$:

\begin{lemma}[Global Sensitivity on Composed Functions]
\label{lemma:restricted_sensitivity_edge}
For projection $\mu: \G _n\to \H_k$ and query $f: \G_n \to \mathbb{R}^m$, define $f_{\H_k} = f \circ \mu$ to be the query applied to the projection. Then $GS_{f_{\H_k}} \leq GS_{\mu} \cdot RS_f(\H_k)$.
\end{lemma}
In particular, this suggests that if we can find a projection $\H_k$ with low global sensitivity $c$, then using $\eps$-differentially private mechanisms like the Laplace mechanism that calibrate noise to $c \cdot RS_f(\H)$ can give significant accuracy gains over global sensitivity. Blocki et al. give such a projection for the edge-adjacency model with $GS_\mu = 3$ that is also efficient (linear in the number of edges in the graph.) We give the details of this projection in \Cref{appendix_projections}.

In the node level-adjacency model an efficient projection with low global sensitivity is not known \cite{KNRS13}. However, it can be shown that if we use the smooth sensitivity of $\mu$, then multiplying this $\beta$-smooth upper bound by the restricted sensitivity of $f$ gives a $\beta$-smooth bound on the local sensitivity of the composition $f_{\H_{k}}$ as above:

\begin{lemma}[$\beta$-Smooth Bound on Composed Functions]
	\label{lemma:restricted_sensitivity_global}
	Let $S_\mu(G)$ be a $\beta$-smooth upper bound on the local sensitivity of $\mu$ on graph $G \in \G_n$. Then $S_{f_{\H_k}} = S_\mu(G) \cdot RS_f(\H_k)$ is a $\beta$-smooth bound on the local sensitivity of $f_{\H_{k}} = f \circ \mu$.
\end{lemma}

We detail two possible projections for the node-adjacency model in in \Cref{appendix_projections} (\cite{KNRS13}, \cite{BBDS13}). The first, which we refer to as $\mu_{trunc}$ simply removes nodes of high degree and the other, $\mu_{LP}$ solves a linear program. It is possible to give $\beta$-smooth upper bounds on the local sensitivity for each of these projections. Roughly speaking, the benefits of node truncation are that it is more efficient than the LP and has low smooth sensitivity when there are few nodes with degree close to the cutoff $k$, which we may assume since degree distributions often follow a power law. However, the smooth sensitivity of $\mu_{trunc}$ could be high for graphs in $\H_k$ if the graph does in fact have many nodes with degree close to the cutoff $k$. On the other hand, the smooth sensitivity of $\mu_{LP}$ is always relatively low when the hypothesis $\H_k$ is true, but the LP is not strictly a projection in that it is guaranteed to project graphs in $\H_k$ to themselves, but its image is $\H_{2k}$ not $\H_k$. Therefore, we must calibrate noise to the restricted sensitivity over $\H_{2k}$ when using the LP. Since, we expect the conditions under which $\mu_{trunc}$ has low smooth sensitivity to be met for network data in practice, it requires lower noise addition from restricted sensitivity and is more efficient than the linear program-based projection we propose using node truncation as the projection for node-adjacency model. 

 Then, taking advantage of restricted sensitivity over $\H_k$ and the appropriate projections, we can perform inference over ERGMs while adding relatively low noise to sufficient statistics that have high global sensitivity.  Our primary focus in proving privacy will be bounding the restricted sensitivity of the queries of interest over $\H_k$ in order to take advantage of this machinery of restricted sensitivity.